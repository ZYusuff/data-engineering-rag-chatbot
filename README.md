# ğŸ¤– The Data Engineer YouTube RAG Chatbot

Welcome to **The Data Engineer YouTube RAG Chatbot**!  
This project is a **Retrieval-Augmented Generation (RAG)** application that allows users to ask questions about **Data Engineering** and receive accurate, context-aware answers based on **AIengerinner YouTube video transcripts**.

The chatbot is designed with the personality of an enthusiastic and experienced data engineer, inspired by educational YouTubers. It combines modern AI tooling with a clean, production-oriented architecture â€” suitable for both academic evaluation and real-world use.

--- 

## Getting Started

Follow the steps below to set up and run the project locally.

### 1. Prerequisites

Make sure you have the following installed:

- **Python 3.12+**
- **Google Gemini API key** (used for embeddings and text generation)

---

### 2. Installation

Clone the repository and install dependencies using `uv`:

```bash
git clone <your-repo-url>
cd data-engineering-rag-chatbot
uv sync
```

### 3. Environment Variables

Create a .env file in the project root and add your API key:

```bash
GEMINI_API_KEY="your_api_key_here"
```
### 4. Ingest Data (Build the Knowledge Base)

Before running the chatbot, the YouTube transcripts must be processed and stored in the vector database.

This step:

- Converts .md files to clean .txt

- Splits text into chunks

- Generates embeddings

- Stores them in LanceDB

Run the ingestion script:

```bash
python backend/ingest_data.py
```

### 5. Run the Application localy

The application consists of a FastAPI backend and a Streamlit frontend. Start them in two separate terminals.

Terminal 1 â€“ Backend (API):

```bash
uv run uvicorn api:app --reload
```

Terminal 2 â€“ Frontend (UI):
```bash
streamlit run app.py
```

### Overview
The application follows a standard RAG architecture:

1. Data Ingestion
YouTube transcripts are cleaned, chunked, and embedded.

2. Vector Storage
LanceDB stores embeddings generated with gemini-embedding-001.

3. RAG Logic
PydanticAI retrieves relevant chunks and generates answers using structured outputs.

![RAG agent](image-2.png)

4. API Layer
FastAPI exposes the chatbot via a POST endpoint.

5. Frontend
Streamlit provides a simple chat interface.

6. Deployment
The API is deployed serverlessly using Azure Functions and consumed by Streamlit locally.

*See the below example of a query answered using Retrieval-Augmented Generation based on YouTube transcripts.*

![Chatbot demo](image.png)

Visual overview of the architecture

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          USER          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ Types question
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Streamlit Frontend  â”‚ (Azure Web App)
        â”‚  (Chat Interface)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ Sends question (HTTP)
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Azure Function     â”‚
        â”‚ (FastAPI / api.py)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ Routes to
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  PydanticAI + Gemini â”‚ (backend/rag.py)
        â”‚  (AI Brain)          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ Uses tool to find
                   â”‚ relevant content
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   LanceDB Vector     â”‚ (knowledge_base/)
        â”‚   Database           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ Finds matching
                   â”‚ transcripts
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Your Transcripts    â”‚ (transcripts/)
        â”‚  (Raw Text Files)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Data Flow:
1. ingestion.py â†’ Reads transcripts â†’ Converts to embeddings â†’ Stores in LanceDB (knowledge_base)
![Data ingestion](image-1.png)
2. User types a question â†’ Streamlit Frontend (Azure Web App)
3. Streamlit sends HTTP request â†’ Azure Function (FastAPI API)
4. Azure Function â†’ RAG Agent (PydanticAI + Gemini)
5. RAG Agent â†’ Searches LanceDB â†’ Retrieves relevant transcript chunks
6. Gemini AI â†’ Generates answer based on retrieved content
7. Answer â†’ Azure Function â†’ Frontend â†’ Displayed to the user

### Tech Stack

A selection of the main tools and technologies used:

LanceDB â€“ Vector database

PydanticAI â€“ Agent framework and structured outputs

Google Gemini â€“ Large Language Model & embeddings

FastAPI â€“ Backend API

Streamlit â€“ Frontend user interface